
<!DOCTYPE html>

<html>
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>Optical Character Recognition Basics &#8212; Teaching Text Analysis with Constellate</title>
    
  <link href="_static/css/theme.css" rel="stylesheet" />
  <link href="_static/css/index.c5995385ac14fb8791e8eb36b4908be2.css" rel="stylesheet" />

    
  <link rel="stylesheet"
    href="_static/vendor/fontawesome/5.13.0/css/all.min.css">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="_static/vendor/fontawesome/5.13.0/webfonts/fa-solid-900.woff2">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="_static/vendor/fontawesome/5.13.0/webfonts/fa-brands-400.woff2">

    
      

    
    <link rel="stylesheet" href="_static/pygments.css" type="text/css" />
    <link rel="stylesheet" href="_static/sphinx-book-theme.css?digest=c3fdc42140077d1ad13ad2f1588a4309" type="text/css" />
    <link rel="stylesheet" type="text/css" href="_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="_static/mystnb.css" />
    <link rel="stylesheet" type="text/css" href="_static/sphinx-thebe.css" />
    <link rel="stylesheet" type="text/css" href="_static/panels-main.c949a650a448cc0ae9fd3441c0e17fb0.css" />
    <link rel="stylesheet" type="text/css" href="_static/panels-variables.06eb56fa6e07937060861dad626602ad.css" />
    
  <link rel="preload" as="script" href="_static/js/index.1c5a1a01449ed65a7b51.js">

    <script id="documentation_options" data-url_root="./" src="_static/documentation_options.js"></script>
    <script src="_static/jquery.js"></script>
    <script src="_static/underscore.js"></script>
    <script src="_static/doctools.js"></script>
    <script src="_static/togglebutton.js"></script>
    <script src="_static/clipboard.min.js"></script>
    <script src="_static/copybutton.js"></script>
    <script >var togglebuttonSelector = '.toggle, .admonition.dropdown, .tag_hide_input div.cell_input, .tag_hide-input div.cell_input, .tag_hide_output div.cell_output, .tag_hide-output div.cell_output, .tag_hide_cell.cell, .tag_hide-cell.cell';</script>
    <script src="_static/sphinx-book-theme.12a9622fbb08dcb3a2a40b2c02b83a57.js"></script>
    <script async="async" src="https://unpkg.com/thebe@0.5.1/lib/index.js"></script>
    <script >
        const thebe_selector = ".thebe"
        const thebe_selector_input = "pre"
        const thebe_selector_output = ".output"
    </script>
    <script async="async" src="_static/sphinx-thebe.js"></script>
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <meta name="docsearch:language" content="en" />
    
  </head>
  <body data-spy="scroll" data-target="#bd-toc-nav" data-offset="80">
    
    <div class="container-fluid" id="banner"></div>

    

    <div class="container-xl">
      <div class="row">
          
<div class="col-12 col-md-3 bd-sidebar site-navigation show" id="site-navigation">
    
        <div class="navbar-brand-box">
    <a class="navbar-brand text-wrap" href="index.html">
      
        <!-- `logo` is deprecated in Sphinx 4.0, so remove this when we stop supporting 3 -->
        
      
      
      <img src="_static/constellate-beta.png" class="logo" alt="logo">
      
      
      <h1 class="site-logo" id="site-title">Teaching Text Analysis with Constellate</h1>
      
    </a>
</div><form class="bd-search d-flex align-items-center" action="search.html" method="get">
  <i class="icon fas fa-search"></i>
  <input type="search" class="form-control" name="q" id="search-input" placeholder="Search this book..." aria-label="Search this book..." autocomplete="off" >
</form><nav class="bd-links" id="bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item active">
        <ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="book/intro.html">
   About
  </a>
 </li>
</ul>
<p class="caption">
 <span class="caption-text">
  The Course
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="book/schedule.html">
   Course Schedule
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="book/syllabus.html">
   Course Syllabus
  </a>
 </li>
</ul>
<p class="caption">
 <span class="caption-text">
  Open Educational Directory
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="book/all-notebooks.html">
   Open Notebook Lessons Directory
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="book/add-my-lesson.html">
   Add your lesson or class
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference external" href="https://constellate.org/docs/how-to-create-a-course-website">
   Create Course from this template
  </a>
 </li>
</ul>
<p class="caption">
 <span class="caption-text">
  Reference
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference external" href="https://constellate.org">
   Constellate Platform
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference external" href="https://constellate.org/docs">
   Constellate Documentation
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference external" href="https://constellate.org/docs/key-terms">
   Text Analysis Glossary
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference external" href="https://jupyterbook.org/intro.html">
   Jupyter Book Documentation
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="book/keep-learning.html">
   More ways to keep learning
  </a>
 </li>
</ul>

    </div>
</nav> <!-- To handle the deprecated key -->

<div class="navbar_extra_footer">
  Powered by <a href="https://jupyterbook.org">Jupyter Book</a>
</div>

</div>


          


          
<main class="col py-md-3 pl-md-4 bd-content overflow-auto" role="main">
    
    <div class="topbar container-xl fixed-top">
    <div class="topbar-contents row">
        <div class="col-12 col-md-3 bd-topbar-whitespace site-navigation show"></div>
        <div class="col pl-md-4 topbar-main">
            
            <button id="navbar-toggler" class="navbar-toggler ml-0" type="button" data-toggle="collapse"
                data-toggle="tooltip" data-placement="bottom" data-target=".site-navigation" aria-controls="navbar-menu"
                aria-expanded="true" aria-label="Toggle navigation" aria-controls="site-navigation"
                title="Toggle navigation" data-toggle="tooltip" data-placement="left">
                <i class="fas fa-bars"></i>
                <i class="fas fa-arrow-left"></i>
                <i class="fas fa-arrow-up"></i>
            </button>
            
            
<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn" aria-label="Download this page"><i
            class="fas fa-download"></i></button>

    <div class="dropdown-buttons">
        <!-- ipynb file if we had a myst markdown file -->
        
        <!-- Download raw file -->
        <a class="dropdown-buttons" href="_sources/ocr-basics.ipynb"><button type="button"
                class="btn btn-secondary topbarbtn" title="Download source file" data-toggle="tooltip"
                data-placement="left">.ipynb</button></a>
        <!-- Download PDF via print -->
        <button type="button" id="download-print" class="btn btn-secondary topbarbtn" title="Print to PDF"
            onClick="window.print()" data-toggle="tooltip" data-placement="left">.pdf</button>
    </div>
</div>

            <!-- Source interaction buttons -->

<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn"
        aria-label="Connect with source repository"><i class="fab fa-github"></i></button>
    <div class="dropdown-buttons sourcebuttons">
        <a class="repository-button"
            href="https://github.com/ithaka/tdm-notebooks"><button type="button" class="btn btn-secondary topbarbtn"
                data-toggle="tooltip" data-placement="left" title="Source repository"><i
                    class="fab fa-github"></i>repository</button></a>
        <a class="issues-button"
            href="https://github.com/ithaka/tdm-notebooks/issues/new?title=Issue%20on%20page%20%2Focr-basics.html&body=Your%20issue%20content%20here."><button
                type="button" class="btn btn-secondary topbarbtn" data-toggle="tooltip" data-placement="left"
                title="Open an issue"><i class="fas fa-lightbulb"></i>open issue</button></a>
        <a class="edit-button" href="https://github.com/ithaka/tdm-notebooks/edit/master/ocr-basics.ipynb"><button
                type="button" class="btn btn-secondary topbarbtn" data-toggle="tooltip" data-placement="left"
                title="Edit this page"><i class="fas fa-pencil-alt"></i>suggest edit</button></a>
    </div>
</div>

            <!-- Full screen (wrap in <a> to have style consistency -->

<a class="full-screen-button"><button type="button" class="btn btn-secondary topbarbtn" data-toggle="tooltip"
        data-placement="bottom" onclick="toggleFullScreen()" aria-label="Fullscreen mode"
        title="Fullscreen mode"><i
            class="fas fa-expand"></i></button></a>

            <!-- Launch buttons -->

<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn"
        aria-label="Launch interactive content"><i class="fas fa-rocket"></i></button>
    <div class="dropdown-buttons">
        
        <a class="binder-button" href="http://backend.constellate.org/binder/launch/v2/gh/ithaka/tdm-notebooks/master?urlpath=tree/ocr-basics.ipynb"><button type="button"
                class="btn btn-secondary topbarbtn" title="Launch Binder" data-toggle="tooltip"
                data-placement="left"><img class="binder-button-logo"
                    src="_static/images/logo_binder.svg"
                    alt="Interact on binder">Binder</button></a>
        
        
        
        
    </div>
</div>

        </div>

        <!-- Table of contents -->
        <div class="d-none d-md-block col-md-2 bd-toc show">
            
            <div class="tocsection onthispage pt-5 pb-3">
                <i class="fas fa-list"></i> Contents
            </div>
            <nav id="bd-toc-nav" aria-label="Page">
                <ul class="visible nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#what-is-ocr-why-is-it-important">
   What is OCR? Why is it important?
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#what-is-ocr">
   What is OCR?
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#ocr-tools">
   OCR Tools
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#questions-when-considering-an-ocr-tool">
     Questions when considering an OCR tool
    </a>
    <ul class="nav section-nav flex-column">
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#proprietary-or-open-source-a-id-proprietary-or-open-a">
       Proprietary or open source?
       <a id="proprietary-or-open">
       </a>
      </a>
     </li>
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#gui-graphical-user-interface-or-script-based-a-id-gui-or-script-a">
       GUI (graphical user interface) or script-based?
       <a id="gui-or-script">
       </a>
      </a>
     </li>
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#file-types-supported-a-id-file-types-a">
       File types supported?
       <a id="file-types">
       </a>
      </a>
     </li>
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#languages-supported-a-id-languages-a">
       Languages supported?
       <a id="languages">
       </a>
      </a>
     </li>
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#which-printed-scripts-can-it-read-a-id-print-scripts-a">
       Which printed scripts can it read?
       <a id="print-scripts">
       </a>
      </a>
     </li>
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#preprocessing-features-a-id-preprocessing-a">
       Preprocessing features?
       <a id="preprocessing">
       </a>
      </a>
     </li>
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#accuracy-and-error-assessment-a-id-accuracy-a">
       Accuracy and error assessment?
       <a id="accuracy">
       </a>
      </a>
     </li>
    </ul>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#popular-ocr-tools">
     Popular OCR Tools
    </a>
    <ul class="nav section-nav flex-column">
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#abbyy-fine-reader">
       ABBYY Fine Reader
      </a>
     </li>
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#adobe-acrobat">
       Adobe Acrobat
      </a>
     </li>
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#amazon-textract">
       Amazon Textract
      </a>
     </li>
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#google-cloud-vision">
       Google Cloud Vision
      </a>
     </li>
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#tesseract">
       Tesseract
      </a>
     </li>
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#pytesseract">
       Pytesseract
      </a>
     </li>
    </ul>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#introduction-to-tesseract">
   Introduction to Tesseract
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#input-files">
     Input Files
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#output-files">
     Output Files
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#pytesseract-basics">
   PyTesseract Basics
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#tesseract-options">
   Tesseract Options
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#ocr-engine-modes-oems">
     OCR Engine Modes (OEMs)
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#page-segmentation-modes-psm">
     Page Segmentation Modes (PSM)
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#file-formats">
     File Formats
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#languages">
     Languages
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#practice-apply-to-your-own-files">
   Practice: Apply to your own files
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#upload-your-selected-text-s-to-the-data-folder-in-your-space-in-the-constellate-analytics-lab">
     1. Upload your selected text(s) to the
     <code class="docutils literal notranslate">
      <span class="pre">
       data/
      </span>
     </code>
     folder in your space in the Constellate Analytics Lab:
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#perform-ocr-on-your-image-file">
     2. Perform OCR on your image file.
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#resources-a-class-anchor-id-resources-a">
   Resources
   <a class="anchor" id="resources">
   </a>
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#jupyter-notebooks-tutorials-reference">
     Jupyter Notebooks Tutorials &amp; Reference
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#readings-on-ocr">
     Readings on OCR
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#ocr-tutorials-reference">
     OCR Tutorials &amp; Reference
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#additional-reading">
     Additional Reading
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#additional-tutorials-thanks-to-everyone-who-contributed">
     Additional Tutorials – Thanks to everyone who contributed!
    </a>
   </li>
  </ul>
 </li>
</ul>

            </nav>
        </div>
    </div>
</div>
    <div id="main-content" class="row">
        <div class="col-12 col-md-9 pl-md-3 pr-md-0">
        
              <div>
                
  <p><img alt="https://ithaka-labs.s3.amazonaws.com/static-files/images/tdm/tdmdocs/CC_BY.png" class="align-left" src="https://ithaka-labs.s3.amazonaws.com/static-files/images/tdm/tdmdocs/CC_BY.png" /><br /></p>
<p>Created by <a class="reference external" href="http://hannahlangstonjacobs.com/">Hannah Jacobs</a> for the <a class="reference external" href="https://nkelber.github.io/tapi2021/book/intro.html">2021 Text Analysis Pedagogy Institute</a>.</p>
<p>Adapted by <a class="reference external" href="http://nkelber.com">Nathan Kelber</a> under <a class="reference external" href="https://creativecommons.org/licenses/by/4.0/">Creative Commons CC BY License</a><br />
For questions/comments/improvements, email <a class="reference external" href="mailto:nathan&#46;kelber&#37;&#52;&#48;ithaka&#46;org">nathan<span>&#46;</span>kelber<span>&#64;</span>ithaka<span>&#46;</span>org</a>.<br /></p>
<hr class="docutils" />
<div class="section" id="optical-character-recognition-basics">
<h1>Optical Character Recognition Basics<a class="headerlink" href="#optical-character-recognition-basics" title="Permalink to this headline">¶</a></h1>
<p>These <a class="reference external" href="https://docs.constellate.org/key-terms/#jupyter-notebook">notebooks</a> describe how to turn images and/or pdf documents into plain text using Tesseract <a class="reference external" href="https://docs.constellate.org/key-terms/#ocr">optical character recognition</a>.</p>
<p><strong>Use Case:</strong> For Learners (Detailed explanation, not ideal for researchers)</p>
<p><strong>Difficulty:</strong> Intermediate</p>
<p><strong>Completion time:</strong> 90 minutes</p>
<p><strong>Knowledge Required:</strong></p>
<ul class="simple">
<li><p>Python Basics (<a class="reference internal" href="python-basics-1.html"><span class="doc std std-doc">Start Python Basics I</span></a>)</p></li>
</ul>
<p><strong>Knowledge Recommended:</strong></p>
<p><strong>Data Format:</strong></p>
<ul class="simple">
<li><p>image files (.jpg, .png)</p></li>
<li><p>document files (.pdf)</p></li>
<li><p>plain text (.txt)</p></li>
</ul>
<p><strong>Libraries Used:</strong></p>
<ul class="simple">
<li><p><a class="reference external" href="https://tesseract-ocr.github.io/">Tesseract</a> for performing <a class="reference external" href="https://docs.constellate.org/key-terms/#ocr">optical character recognition</a>.</p></li>
</ul>
<p><strong>Learning Objectives:</strong>
By the end of this lessons, students will be able to</p>
<ol class="simple">
<li><p>Define “OCR”</p></li>
<li><p>Explain the importance of OCR for computer-aided reading and analysis</p></li>
<li><p>Perform basic OCR operations using Python, Tesseract, and Jupyter Notebooks</p></li>
</ol>
<p><strong>Research Pipeline:</strong></p>
<ol class="simple">
<li><p>Digitize documents</p></li>
<li><p><strong>Optical Character Recognition</strong></p></li>
<li><p>Tokenize your texts</p></li>
<li><p>Perform analysis</p></li>
</ol>
<div class="section" id="what-is-ocr-why-is-it-important">
<h2>What is OCR? Why is it important?<a class="headerlink" href="#what-is-ocr-why-is-it-important" title="Permalink to this headline">¶</a></h2>
<p>In order to do text analysis (or <a class="reference external" href="https://docs.constellate.org/key-terms/#nlp">natural language processing</a>, we need to have our text in a machine-readable format such as plaintext. In practice, this usually means converting an image file (e.g. a file ending in .png or jpg) into a plaintext file (.txt). Text is machine-readable if you are able to select, copy, and paste it’s individual characters.</p>
<p>The difference can be illustrated by a digital image (.png) of the print edition of Dr. Faust.</p>
<p><img alt="An image of the German print edition of Dr. Faust" src="https://ithaka-labs.s3.amazonaws.com/static-files/images/tdm/tdmdocs/faust.png" /></p>
<p>and the <a class="reference external" href="https://www.gutenberg.org/files/2229/2229-0.txt">text version found on Project Gutenberg</a>. While a human can read the text of the digital image, a computer is not able to manipulate the individual characters of the text. The digital text cannot be easily copied and pasted for manipulation in other applications.</p>
<p><img alt="Image of the text &quot;Blackwell's&quot; showing the pixels." src="https://ithaka-labs.s3.amazonaws.com/static-files/images/tdm/tdmdocs/blackwell-pixels.jpeg" /></p>
<p>While we might see this as the word <code class="docutils literal notranslate"><span class="pre">Blackwell's</span></code>, the computer understands the above as a series of squares, <strong>pixels</strong>, containing information about which color the pixel should be–<em>not</em> which character to display.  If we want the computer to be able to work this text <em>as</em> text, we need to convert the image above into this:</p>
<p><code class="docutils literal notranslate"><span class="pre">01000010</span> <span class="pre">01101100</span> <span class="pre">01100001</span> <span class="pre">01100011</span> <span class="pre">01101011</span> <span class="pre">01110111</span> <span class="pre">01100101</span> <span class="pre">01101100</span> <span class="pre">01101100</span> <span class="pre">00100111</span> <span class="pre">01110011</span></code></p>
<p>…which the computer will then display for human readers as <code class="docutils literal notranslate"><span class="pre">Blackwell's</span></code>. We can then use our computers to search for instances of this word, analyze its freqency, patterns in occurrence, collocation, and so on. We can also ask the computer to read this and any other words in the page aloud if we need to hear them instead of viewing them on a screen.</p>
</div>
<hr class="docutils" />
<div class="section" id="what-is-ocr">
<h2>What is OCR?<a class="headerlink" href="#what-is-ocr" title="Permalink to this headline">¶</a></h2>
<p>OCR, or “Optical Character Recognition,” is <strong>a computational process that converts digital images of text into computer-readable text</strong>. OCR is both a noun and a verb.</p>
<p>More specifically:</p>
<blockquote>
<div><p><strong>OCR software attempts to replicate the combined functions of the human eye and brain, which is why it is referred to as artificial intelligence software.</strong> A human can quickly and easily recognise text of varying fonts and of various print qualities on a newspaper page, and will apply their language and cognitive abilities to correctly translate this text into meaningful words. Humans can recognise, translate and interpret the text on a newspaper page very rapidly, even text on an old poor quality newspaper page from the 1800s. We can quickly scan layout, sections and headings, and read the text of articles in the right order (which is much more difficult than reading the page of a book). <strong>OCR software can now do all these things too, but not to the same level of perfection as a human can.</strong> - (<a class="reference external" href="http://www.dlib.org/dlib/march09/holley/03holley.html">Holley, “How Good Can It Get? Analysing and Improving OCR Accuracy in Large Scale Historic Newspaper Digitisation Programs”</a>).</p>
</div></blockquote>
<blockquote>
<div><p>“Optical character recognition (OCR) software is <strong>a type of artificial intelligence software designed to mimic the functions of the human eye and brain and discern which marks within an image represent letterforms or other markers of written language.</strong> OCR scans an image for semantically-meaningful material and transcribes what language it finds into text data.” - <a class="reference external" href="https://ryancordell.org/research/why-ocr/">Cordell, “Why You (A Humanist) Should Care About Optical Character Recognition”</a>.</p>
</div></blockquote>
</div>
<div class="section" id="ocr-tools">
<h2>OCR Tools<a class="headerlink" href="#ocr-tools" title="Permalink to this headline">¶</a></h2>
<p>If you have <a class="reference external" href="https://acrobat.adobe.com/us/en/">Adobe Acrobat</a> on your computer, then you have probably already been using software that contains OCR functionality. Acrobat’s OCR is designed to help users <a class="reference external" href="https://helpx.adobe.com/acrobat/using/edit-scanned-pdfs.html">edit scanned PDFs or PDFs created by others</a>. It can also be used to export editable text versions (e.g. Microsoft Word documents), or to ask the computer to read aloud the text contained in the PDF. However, <em>at scale</em> and working with <em>older printed documents, perhaps with irregular printing patterns</em>, Acrobat may not give you the best results.</p>
<div class="section" id="questions-when-considering-an-ocr-tool">
<h3>Questions when considering an OCR tool<a class="headerlink" href="#questions-when-considering-an-ocr-tool" title="Permalink to this headline">¶</a></h3>
<ul class="simple">
<li><p><a class="reference external" href="#proprietary-or-open">Proprietary or open source?</a></p></li>
<li><p><a class="reference external" href="#gui-or-script">GUI (graphical user interface) or script-based?</a></p></li>
<li><p><a class="reference external" href="#file-types">File types supported?</a></p></li>
<li><p><a class="reference external" href="#languages">Languages supported?</a></p></li>
<li><p><a class="reference external" href="#print-scripts">Which printed scripts can it read?</a></p></li>
<li><p><a class="reference external" href="#preprocessing">Preprocessing features?</a></p></li>
<li><p><a class="reference external" href="#accuracy">Accuracy and error assessment?</a></p></li>
</ul>
<p>There may be other questions you’ll need to add to this list, but it will get you started. Likewise, you may wish to reorder these questions based on your project’s priorities.</p>
<div class="section" id="proprietary-or-open-source-a-id-proprietary-or-open-a">
<h4>Proprietary or open source? <a id="proprietary-or-open"></a><a class="headerlink" href="#proprietary-or-open-source-a-id-proprietary-or-open-a" title="Permalink to this headline">¶</a></h4>
<p>Proprietary, meaning do you need to purchase a license? Knowing the resources you have or need to start your OCR project is key to how you make your decision. You may wish to work with a program such as <a class="reference external" href="https://pdf.abbyy.com/pricing/">ABBYY FineReader</a>, which includes a number of graphical features for preprocessing that you’d like to use. But you’ll need to be prepared to pay $200-300 for it. If you don’t have those funds, you may wish to work with a free tool.</p>
<p>Although <em>free</em> software is not necessarily the same as <em>open source</em> software, <a class="reference external" href="https://en.wikipedia.org/wiki/Open-source_model">open source</a> software is free. <strong>Open source, in the software world, refers to software whose creators have made the underlying code available for others to edit and build upon.</strong> You may opt to choose an open source OCR tool so that you have more access to the codebase, and therefore better understanding of the computation that goes into performing OCR on your corpora.</p>
</div>
<div class="section" id="gui-graphical-user-interface-or-script-based-a-id-gui-or-script-a">
<h4>GUI (graphical user interface) or script-based?<a id="gui-or-script"></a><a class="headerlink" href="#gui-graphical-user-interface-or-script-based-a-id-gui-or-script-a" title="Permalink to this headline">¶</a></h4>
<p>If you are working on a project alone with no coding experience, you may be thinking that a GUI that provides the ease of clicking a button is the best way to go–and it may be if you have a small set of documents with modern typefaces.</p>
<p>On the other hand, you may wish to learn some coding if there are a significant number of documents and/or those documents contain unusual features (typefaces, language, text layouts, etc.). If so, learning how to run OCR with Python is a great opportunity. Even if you’re collaborating with a programmer who will write most of your OCR code, you may want to learn some of the concepts and basic steps behind the OCR to ensure you have a good understanding of this project phase and to aid communications with your collaborator.</p>
</div>
<div class="section" id="file-types-supported-a-id-file-types-a">
<h4>File types supported?<a id="file-types"></a><a class="headerlink" href="#file-types-supported-a-id-file-types-a" title="Permalink to this headline">¶</a></h4>
<p>Does the OCR tool work only with PDFs, or can it also read image files? Which file type(s) are you working with? This may seem a small point, but if you have image files, and you purchase a license for OCR software that works only with PDFs, you may be a bit surprised. There are tools out there that can help you convert images to PDFs, but you may risk degrading the scanned text with these conversions.</p>
</div>
<div class="section" id="languages-supported-a-id-languages-a">
<h4>Languages supported?<a id="languages"></a><a class="headerlink" href="#languages-supported-a-id-languages-a" title="Permalink to this headline">¶</a></h4>
<p>If you are working with texts that are not in English, it’s a good idea to check. At this point, most OCR tools work with multiple languages.</p>
</div>
<div class="section" id="which-printed-scripts-can-it-read-a-id-print-scripts-a">
<h4>Which printed scripts can it read?<a id="print-scripts"></a><a class="headerlink" href="#which-printed-scripts-can-it-read-a-id-print-scripts-a" title="Permalink to this headline">¶</a></h4>
<p>If you’re working with a language written in a script no longer commonly in use, you may need to seek out some specific tools to assist you. Even if you’re working with <a class="reference external" href="https://chroniclingamerica.loc.gov/lccn/sn93060356/1917-01-18/ed-1/seq-1/#date1=1880&amp;index=11&amp;date2=1917&amp;searchType=advanced&amp;language=&amp;sequence=0&amp;words=son+sonille&amp;proxdistance=5&amp;state=Missouri&amp;rows=20&amp;ortext=son&amp;proxtext=&amp;phrasetext=&amp;andtext=&amp;dateFilterType=yearRange&amp;page=1">late-nineteenth- and early-twentieth-century American non-English newspapers</a>, you may need to find out which tools handle specific scripts.</p>
</div>
<div class="section" id="preprocessing-features-a-id-preprocessing-a">
<h4>Preprocessing features?<a id="preprocessing"></a><a class="headerlink" href="#preprocessing-features-a-id-preprocessing-a" title="Permalink to this headline">¶</a></h4>
<p><strong>Preprocessing is a set of steps that we can use to try to minimize issues such as a skewed page, faded text, or smudges on a page <em>before</em> performing OCR.</strong> Some OCR tools offer some preprocessing tools. Others don’t. Even if a tool can run preprocessing, though, you may find you have a specific need that must be met with another tool.</p>
</div>
<div class="section" id="accuracy-and-error-assessment-a-id-accuracy-a">
<h4>Accuracy and error assessment?<a id="accuracy"></a><a class="headerlink" href="#accuracy-and-error-assessment-a-id-accuracy-a" title="Permalink to this headline">¶</a></h4>
<p>Can the tool help you evaluate how well the process has gone and where there may be errors to correct? Are there tools to support both automated and manual error correction? How will you know if the OCRed corpus you’ve produced is of a high enough quality?</p>
</div>
</div>
<div class="section" id="popular-ocr-tools">
<h3>Popular OCR Tools<a class="headerlink" href="#popular-ocr-tools" title="Permalink to this headline">¶</a></h3>
<div class="section" id="abbyy-fine-reader">
<h4><a class="reference external" href="https://pdf.abbyy.com/">ABBYY Fine Reader</a><a class="headerlink" href="#abbyy-fine-reader" title="Permalink to this headline">¶</a></h4>
<p>Perhaps at the opposite end of the OCR spectrum from Pytesseract, ABBYY is another powerful OCR tool. It has a GUI (graphical user interface) in which users can make adjustments (preprocessing), and it also has an SDK (software developer toolkit) that programmers can use to run ABBYY tools in their own programs. ABBYY even has a cloud service. Like Tesseract, ABBYY supports many languages and a number of file formats. ABBYY is, however, proprietary–you’ll need to be prepared to pay a minimum of $200 if your institution does not provide a license.</p>
</div>
<div class="section" id="adobe-acrobat">
<h4><a class="reference external" href="https://acrobat.adobe.com/us/en/acrobat.html">Adobe Acrobat</a><a class="headerlink" href="#adobe-acrobat" title="Permalink to this headline">¶</a></h4>
<p>A common PDF reader, Acrobat can do a lot of things including OCR. It comes in DC and Pro DC versions, and both are paid. DC includes OCR functionality in the “Enhance PDF” menu.</p>
</div>
<div class="section" id="amazon-textract">
<h4><a class="reference external" href="https://aws.amazon.com/textract/resources/?blog-posts-cards.sort-by=item.additionalFields.createdDate&amp;blog-posts-cards.sort-order=desc">Amazon Textract</a><a class="headerlink" href="#amazon-textract" title="Permalink to this headline">¶</a></h4>
<p>Like Pytesseract, this tool from Amazon runs in Python. Like ABBYY Fine Reader, it’s proprietary code, which means we don’t know what’s happening in Textract itself when we use it–it’s a black box. There is a free tier to get started if you’re working with fewer than 1,000 pages, and you can run your Textract code in Amazon’s cloud environment. The cost to use it, if you are planning to learn a little programming or are working with a programmer, is significantly lower than the cost of an ABBYY license.</p>
</div>
<div class="section" id="google-cloud-vision">
<h4><a class="reference external" href="https://cloud.google.com/vision/docs">Google Cloud Vision</a><a class="headerlink" href="#google-cloud-vision" title="Permalink to this headline">¶</a></h4>
<p>A competitor of Amazon’s, Google’s Cloud Vision API (application programming interface) is likewise proprietary after a certain number of uses, requires programming knowledge, and can be used in the cloud. This same tool can be used to perform computer vision tasks such as facial recognition. Because we don’t know what’s happening in Cloud Vision’s code when we use it, we might not be able to explain unexpected results–it’s another <a class="reference external" href="01-AlgorithmsOfResistance-WhatIsAnAlgorithm.ipynb#algorithms">black box</a>.</p>
</div>
<div class="section" id="tesseract">
<h4><a class="reference external" href="https://tesseract-ocr.github.io/tessdoc/Home.html">Tesseract</a><a class="headerlink" href="#tesseract" title="Permalink to this headline">¶</a></h4>
<p>An OCR engine (basically, a collection of algorithms and training data) originally developed by Hewlett Packard and maintained by Google. Tesseract is open source and supports many languages and scripts. It also offers possibilities to customize OCR outputs in ways that may or may not be possible with proprietary software. The ability to add your own training data is also a big feature, though a resource-intensive process. Programmers have taken advantage of Tesseract being open source and have created <a class="reference external" href="https://tesseract-ocr.github.io/tessdoc/User-Projects-%E2%80%93-3rdParty">a number of tools based on Tesseract</a> (some with GUIs).</p>
</div>
<div class="section" id="pytesseract">
<h4><a class="reference external" href="https://pypi.org/project/pytesseract/">Pytesseract</a><a class="headerlink" href="#pytesseract" title="Permalink to this headline">¶</a></h4>
<p>Pytesseract (or Python-tesseract) is a powerful OCR tool made for the programming language Python using the Tesseract OCR Engine. It can work with many file formats and (human) languages, and, like <a class="reference external" href="https://github.com/tesseract-ocr/tesseract">Tesseract</a>, is open source. Since Pytesseract is used in a larger programming ecosystem, it can be combined with a variety of other Python packages to perform many different tasks. Furthermore, Python is both highly used and a popular computer language for beginning programmers, making it possible for users to move quickly from the basics of Python into working with Pytesseract.</p>
</div>
</div>
</div>
<div class="section" id="introduction-to-tesseract">
<h2>Introduction to Tesseract<a class="headerlink" href="#introduction-to-tesseract" title="Permalink to this headline">¶</a></h2>
<p><a class="reference external" href="https://github.com/tesseract-ocr/tesseract">Tesseract</a> was initially developed by Hewlett-Packward between 1985-1994. HP made it open source in 2005. <a class="reference external" href="https://opensource.google/projects/tesseract">Google developed it</a> between 2006-2018. It is still open source and maintained Zdenko Podobny. There is an <a class="reference external" href="https://groups.google.com/g/tesseract-ocr">active user forum</a>.</p>
<p>Tesseract supports over <a class="reference external" href="https://tesseract-ocr.github.io/tessdoc/Data-Files-in-different-versions.html">100 languages</a> and can be <a class="reference external" href="https://github.com/tesseract-ocr/tesseract#running-tesseract">run in the command line</a> on Windows, MacOS, and Linux. Its outputs can be stored in several interoperable file formats. There are a number of <a class="reference external" href="https://tesseract-ocr.github.io/tessdoc/User-Projects-%E2%80%93-3rdParty.html">third party GUIs available</a>.</p>
<p>The latest versions (4x) of Tesseract incorporate <a class="reference external" href="https://en.wikipedia.org/wiki/Long_short-term_memory">LSTM (Long Short-Term Memory)</a>, an artificial Recurrent Neural Network. LSTM is a set of algorithms that computers can run to process lots of data, “remember” that data, and apply what it “learns” from that data to other data as it’s processing.</p>
<p>Because Tesseract is free and open source, it’s <a class="reference external" href="https://tesseract-ocr.github.io/tessdoc/tess4/TrainingTesseract-4.00.html">possible to retrain Tesseract in order to OCR a specific corpus</a>. This requires a large and specific dataset, some expertise, and some time. But it’s a key feature that you won’t get from proprietary or closed-source software.</p>
<p><a class="reference external" href="https://pypi.org/project/pytesseract/">PyTesseract</a> is a “wrapper” – basically it makes Tesseract legible to Python so that it can be incorporated into various Python environments and functionalities. This means that if you’re already working in Python, you don’t need to leave your environment to build a dataset. You could also build PyTesseract into a Python application and/or into a code base that you plan to reuse. It was <a class="reference external" href="https://github.com/madmaze/pytesseract">developed and maintained</a> beginning in 2014 by a group of programmers led by Mattias Lee.</p>
<div class="section" id="input-files">
<h3>Input Files<a class="headerlink" href="#input-files" title="Permalink to this headline">¶</a></h3>
<p>In order to perform OCR on a text corpus, we need the following:</p>
<ul class="simple">
<li><p>A <strong>single file folder</strong> containing all of the corpus files. If the corpus is small enough (e.g. 1 book), this could be simply a single file (e.g. a .pdf).</p></li>
<li><p>All corpus files should be of the <strong>same file format</strong>.</p></li>
<li><p>The chosen file format should be <strong>interoperable</strong> (usable by many software and operating systems) and stable (changes rarely if ever).</p></li>
<li><p>For our work with Python and Tesseract, the files should be <strong>images</strong>, which means that each file will correspond to 1 single-sided page (recto or verso, assuming a book format).</p></li>
</ul>
<p><img alt="First page of the 1955 North Carolina Session Laws" src="https://ithaka-labs.s3.amazonaws.com/static-files/images/tdm/tdmdocs/sessionlaw-example.jpeg" /></p>
<p><strong>To keep image files organized,</strong> it is helpful to create a file structure where every book is within a unique folder. Each book’s folder then contains a series of numbered images for each page.</p>
<p><img alt="Screenshot of a file structure for image files to be OCR'ed." src="https://ithaka-labs.s3.amazonaws.com/static-files/images/tdm/tdmdocs/folder-structure.jpeg" /></p>
<p>Note that the file naming structure identifies <em>both</em> which volume the images are part of <em>and</em> which scanned page they correspond to, which helps us maintain the order of the volume. These numbers <em>may not</em> correspond to page numbers because bookscanning usually includes the outer and inner covers, title pages, and other book pages that are not usually numbered.</p>
<p>Note that we are working with .jpg files here. The process we’ll be using, though, can also be run with .png, .tiff, .jp2, and other common interoperable image formats.</p>
</div>
<div class="section" id="output-files">
<h3>Output Files<a class="headerlink" href="#output-files" title="Permalink to this headline">¶</a></h3>
<p>For each folder of files (whether .jpg, .png, or .pdf), we will create a single plaintext file (.txt) that contains the full-text.  The plain text file format is interoperable, stable, and fully computer readable, meaning it will be ready for performing computational analysis and for storing in repositories and databases.</p>
</div>
</div>
<div class="section" id="pytesseract-basics">
<h2>PyTesseract Basics<a class="headerlink" href="#pytesseract-basics" title="Permalink to this headline">¶</a></h2>
<p>Here we will describe the basic process of OCRing using PyTesseract. The first step is to install Tesseract on your machine using the command line. The following code cell installs Tesseract-OCR on the command line.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="o">%%</span><span class="k">bash</span>
apt install tesseract-ocr
y
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Install PyTesseract, the Python wrapper for Tesseract</span>
<span class="c1"># An exclamation point runs the command on the command line</span>
<span class="o">!</span>pip install pytesseract
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Install Tesseract training data in the Constellate Analytics Lab.</span>
<span class="c1"># The exclamation runs the command as a terminal command.</span>

<span class="o">!</span>wget https://github.com/tesseract-ocr/tessdata/raw/main/eng.traineddata
<span class="o">!</span>mv eng.traineddata /usr/share/tesseract-ocr/4.00/tessdata/eng.traineddata
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Download the Sample Page Images for this lesson</span>
<span class="c1"># Change the list `download_urls` to bring in other documents</span>

<span class="kn">import</span> <span class="nn">urllib.request</span>
<span class="n">download_urls</span> <span class="o">=</span> <span class="p">[</span>
    <span class="s1">&#39;https://ithaka-labs.s3.amazonaws.com/static-files/images/tdm/tdmdocs/ocr_sample.jpg&#39;</span><span class="p">,</span>
    <span class="s1">&#39;https://ithaka-labs.s3.amazonaws.com/static-files/images/tdm/tdmdocs/cien-an%CC</span><span class="si">%83o</span><span class="s1">s-de-soledad.png&#39;</span>
<span class="p">]</span>

<span class="k">for</span> <span class="n">url</span> <span class="ow">in</span> <span class="n">download_urls</span><span class="p">:</span>
    <span class="n">urllib</span><span class="o">.</span><span class="n">request</span><span class="o">.</span><span class="n">urlretrieve</span><span class="p">(</span><span class="n">url</span><span class="p">,</span> <span class="s1">&#39;./data/&#39;</span> <span class="o">+</span> <span class="n">url</span><span class="o">.</span><span class="n">rsplit</span><span class="p">(</span><span class="s1">&#39;/&#39;</span><span class="p">,</span> <span class="mi">1</span><span class="p">)[</span><span class="o">-</span><span class="mi">1</span><span class="p">])</span>
</pre></div>
</div>
</div>
</div>
<p>We will convert a <span class="xref myst">sample .jpg image</span> to text. The sample comes from the Session Laws of the State of North Carolina. The material was OCRed for the <a class="reference external" href="https://www.neh.gov/">NEH-funded</a>, <a class="reference external" href="https://collectionsasdata.github.io/">Collections as Data</a> project <a class="reference external" href="https://onthebooks.lib.unc.edu/">On the Books: Jim Crow and Algorithms of Resistance</a>.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Import the Image module from the Pillow Library, which will help us access the image.</span>
<span class="kn">from</span> <span class="nn">PIL</span> <span class="kn">import</span> <span class="n">Image</span>

<span class="c1"># Import the pytesseract library, which will run the OCR process.</span>
<span class="kn">import</span> <span class="nn">pytesseract</span>

<span class="c1"># Open a specific image file, convert the text in the image to computer-readable text (OCR),</span>
<span class="c1"># and then print the results for us to see here.</span>
<span class="nb">print</span><span class="p">(</span><span class="n">pytesseract</span><span class="o">.</span><span class="n">image_to_string</span><span class="p">(</span><span class="n">Image</span><span class="o">.</span><span class="n">open</span><span class="p">(</span><span class="s2">&quot;./data/ocr_sample.jpg&quot;</span><span class="p">),</span> <span class="n">lang</span><span class="o">=</span><span class="s2">&quot;eng&quot;</span><span class="p">))</span>
</pre></div>
</div>
</div>
</div>
<p>Let’s break down the above code, from the inside out:</p>
<ol class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">Image.open(&quot;./data/ocr_sample.jpg&quot;),</span> <span class="pre">lang=&quot;eng&quot;)</span></code> - Open the image file <code class="docutils literal notranslate"><span class="pre">ocr_sample.jeg</span></code> in the <code class="docutils literal notranslate"><span class="pre">/data</span></code> folder. Set the language to English.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">pytesseract.image_to_string()</span></code> - Using PyTesseract’s <code class="docutils literal notranslate"><span class="pre">image_to_string</span></code> function, detect alphanumeric characters in the image and convert them into computer-readable text.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">print()</span></code> - Display the computer-readable text output.</p></li>
</ol>
</div>
<div class="section" id="tesseract-options">
<h2>Tesseract Options<a class="headerlink" href="#tesseract-options" title="Permalink to this headline">¶</a></h2>
<p>Tesseract offers a number of different modes, or settings, that we can use to customize output. There are two types of modes: OEMs (OCR Engine Modes), which specify which OCR tools are available to Tesseract to use, and PSMs (Page Segmentations Modes), which specify how the OCR tools should read the image files–how to separate and order sections of text in the image file.</p>
<div class="section" id="ocr-engine-modes-oems">
<h3>OCR Engine Modes (OEMs)<a class="headerlink" href="#ocr-engine-modes-oems" title="Permalink to this headline">¶</a></h3>
<p>Run the following command to view the list of OEMs.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># List the Tesseract OCR Engine Modes (OEMs)</span>
<span class="c1"># Run a terminal command using an exclamation point</span>
<span class="o">!</span>tesseract --help-oem
</pre></div>
</div>
</div>
</div>
<p>Here’s more of an explanation of OCR Engine Modes (OEMs):</p>
<ul class="simple">
<li><p><em>0 - Original Tesseract only.</em> - This mode runs only the main Tesseract mode.</p></li>
<li><p><em>1 - Cube only.</em> - This mode runs only Cube, <a class="reference external" href="https://code.google.com/archive/p/tesseract-ocr-extradocs/wikis/Cube.wiki">according to Google</a>, “an alternative recognition mode for Tesseract. It is slower than the original recognition engine, but often produces better results.” <a class="reference external" href="https://nanonets.com/blog/ocr-with-tesseract/">A Nanonets tutorial explains</a> that this is the LSTM mode. There is not much documentation out about this.</p></li>
<li><p><em>2 - Tesseract + Cube.</em> - Both Tesseract (Nanonets refers to this as “Legacy”) and Cube (LSTM) modes are used.</p></li>
<li><p><em>3 - Default, based on what is available.</em> - Tesseract will choose an OEM based on the configurations (language, PSM) we give it. Even if we don’t include the configuration information, Tesseract will run in OEM 3.</p></li>
</ul>
<hr class="docutils" />
<h3 style="color:red; display:inline">Try it! &lt; / &gt; </h3>
<p>Run the following script, trying each of the different OEMs in turn replace the number in the first line to change the OEM.</p>
<hr class="docutils" />
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Change the OEM number below to try</span>
<span class="c1"># running another OCR mode.</span>
<span class="c1"># 3 is the default setting.</span>
<span class="n">custom_oem_config</span> <span class="o">=</span> <span class="sa">r</span><span class="s1">&#39;--oem 3&#39;</span>

<span class="c1"># Open a specific image file, convert the text in the image to computer-readable text (OCR)</span>
<span class="c1"># following the language and mode configuration we specify,</span>
<span class="c1"># and then print the results for us to see here.</span>
<span class="nb">print</span><span class="p">(</span><span class="n">pytesseract</span><span class="o">.</span><span class="n">image_to_string</span><span class="p">(</span><span class="n">Image</span><span class="o">.</span><span class="n">open</span><span class="p">(</span><span class="s2">&quot;./data/ocr_sample.jpg&quot;</span><span class="p">),</span> <span class="n">lang</span><span class="o">=</span><span class="s2">&quot;eng&quot;</span><span class="p">,</span> <span class="n">config</span><span class="o">=</span><span class="n">custom_oem_config</span><span class="p">))</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="section" id="page-segmentation-modes-psm">
<h3>Page Segmentation Modes (PSM)<a class="headerlink" href="#page-segmentation-modes-psm" title="Permalink to this headline">¶</a></h3>
<p>Run the following command to view all of the PSMs:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="o">!</span>tesseract --help-psm
</pre></div>
</div>
</div>
</div>
<p>This time, our configuration looks like</p>
<p><code class="docutils literal notranslate"><span class="pre">custom_psm_config</span> <span class="pre">=</span> <span class="pre">r'--psm</span> <span class="pre">3'</span></code></p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Change the PSM number below to try</span>
<span class="c1"># running another page segmentation mode.</span>
<span class="c1"># 3 is the default setting.</span>
<span class="n">custom_psm_config</span> <span class="o">=</span> <span class="sa">r</span><span class="s1">&#39;--psm 3&#39;</span>

<span class="c1"># Open a specific image file, convert the text in the image to computer-readable text (OCR)</span>
<span class="c1"># following the language and mode configuration we specify,</span>
<span class="c1"># and then print the results for us to see here.</span>
<span class="nb">print</span><span class="p">(</span><span class="n">pytesseract</span><span class="o">.</span><span class="n">image_to_string</span><span class="p">(</span><span class="n">Image</span><span class="o">.</span><span class="n">open</span><span class="p">(</span><span class="s2">&quot;./data/ocr_sample.jpg&quot;</span><span class="p">),</span> <span class="n">lang</span><span class="o">=</span><span class="s2">&quot;eng&quot;</span><span class="p">,</span> <span class="n">config</span><span class="o">=</span><span class="n">custom_psm_config</span><span class="p">))</span>
</pre></div>
</div>
</div>
</div>
<p>Many of the PSMs are meant for images that have little text in them – such as images that include road or store signs. <a class="reference external" href="https://tesseract-ocr.github.io/tessdoc/ImproveQuality">See Tesseract’s documentation on improving OCR quality.</a></p>
<p><strong>Most of the time, the default OEM and PSM is best.</strong> There may be times when you are working with materials for which experimenting with these options may be useful.</p>
<p>Note that it’s possible to customize the <code class="docutils literal notranslate"><span class="pre">oem</span></code> and <code class="docutils literal notranslate"><span class="pre">psm</span></code> together. Here’s how:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Change the numbers below to try</span>
<span class="c1"># running other modes together.</span>
<span class="n">custom_oem_psm_config</span> <span class="o">=</span> <span class="sa">r</span><span class="s1">&#39;--oem 3 --psm 4&#39;</span>

<span class="c1"># Open a specific image file, convert the text in the image to computer-readable text (OCR)</span>
<span class="c1"># following the language and mode configuration we specify,</span>
<span class="c1"># and then print the results for us to see here.</span>
<span class="nb">print</span><span class="p">(</span>
    <span class="n">pytesseract</span><span class="o">.</span><span class="n">image_to_string</span><span class="p">(</span>
        <span class="n">Image</span><span class="o">.</span><span class="n">open</span><span class="p">(</span><span class="s2">&quot;./data/ocr_sample.jpg&quot;</span><span class="p">),</span>
        <span class="n">lang</span><span class="o">=</span><span class="s2">&quot;eng&quot;</span><span class="p">,</span>
        <span class="n">config</span><span class="o">=</span><span class="n">custom_oem_psm_config</span><span class="p">)</span>
<span class="p">)</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="section" id="file-formats">
<h3>File Formats<a class="headerlink" href="#file-formats" title="Permalink to this headline">¶</a></h3>
<p>In addition to .txt, Tesseract can convert OCR’ed images into <a class="reference external" href="https://en.wikipedia.org/wiki/HOCR">hOCR (HTML)</a>, searchable PDF, and TSV.</p>
<hr class="docutils" />
<h3 style="color:red; display:inline">Try it! &lt; / &gt; </h3>
<p>The scripts below output various file formats. Try each and then click the file link below each script to view the output. You’ll also find the files by clicking on the Jupyter icon at the top of this window.</p>
<hr class="docutils" />
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Output to text file (.txt)</span>

<span class="c1"># File location</span>
<span class="c1"># You can change the filename in quotes below to OCR a different file.</span>
<span class="n">file</span> <span class="o">=</span> <span class="s2">&quot;./data/ocr_sample.jpg&quot;</span>

<span class="c1"># Open the file named above. </span>
<span class="c1"># While it&#39;s open, do several things:</span>
<span class="k">with</span> <span class="nb">open</span><span class="p">(</span><span class="n">file</span><span class="p">,</span> <span class="s1">&#39;rb&#39;</span><span class="p">)</span> <span class="k">as</span> <span class="n">inputFile</span><span class="p">:</span>
        
    <span class="c1"># Read the file using PIL&#39;s Image module.</span>
    <span class="n">img</span> <span class="o">=</span> <span class="n">Image</span><span class="o">.</span><span class="n">open</span><span class="p">(</span><span class="n">inputFile</span><span class="p">)</span>
    
    <span class="c1"># Run OCR on the open file.</span>
    <span class="n">ocrText</span> <span class="o">=</span> <span class="n">pytesseract</span><span class="o">.</span><span class="n">image_to_string</span><span class="p">(</span><span class="n">img</span><span class="p">)</span>
        
<span class="c1"># Get a file name--without the extension-- </span>
<span class="c1"># to use when we name the output file.</span>
    
<span class="n">fileName</span> <span class="o">=</span> <span class="n">file</span><span class="o">.</span><span class="n">rsplit</span><span class="p">(</span><span class="s1">&#39;/&#39;</span><span class="p">,</span> <span class="mi">1</span><span class="p">)[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>
<span class="n">fileName</span> <span class="o">=</span> <span class="n">fileName</span><span class="o">.</span><span class="n">rsplit</span><span class="p">(</span><span class="s1">&#39;.&#39;</span><span class="p">,</span> <span class="mi">1</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span>

<span class="c1"># The image file above will be closed before moving on to this line.</span>
<span class="c1"># The OCR&#39;ed text has been pulled from the image and stored in</span>
<span class="c1"># a Python variable for us to continue to use.</span>

<span class="c1"># Create and open a new text file, name it to match its input file,</span>
<span class="c1"># declare its encoding to be UTF-8 so that it correctly outputs</span>
<span class="c1"># non-ASCII characters.</span>
<span class="k">with</span> <span class="nb">open</span><span class="p">(</span><span class="n">fileName</span> <span class="o">+</span> <span class="s2">&quot;.txt&quot;</span><span class="p">,</span> <span class="s2">&quot;w&quot;</span><span class="p">,</span> <span class="n">encoding</span><span class="o">=</span><span class="s2">&quot;utf-8&quot;</span><span class="p">)</span> <span class="k">as</span> <span class="n">outFile</span><span class="p">:</span>
        
    <span class="c1"># and write the OCR&#39;ed text to the file.</span>
    <span class="n">outFile</span><span class="o">.</span><span class="n">write</span><span class="p">(</span><span class="n">ocrText</span><span class="p">)</span>

<span class="c1"># Display a message to let us know the file has been created</span>
<span class="c1"># and the script successfully completed.</span>
<span class="nb">print</span><span class="p">(</span><span class="n">fileName</span><span class="p">,</span> <span class="s2">&quot;text file successfully created.&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p>To open this file, use the file menu to select: <strong>File &gt; Open</strong>. You may then choose to view, edit, modify, or download the file to your local machine.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Output to pdf file (.pdf)</span>

<span class="c1"># File location</span>
<span class="c1"># You can change the filename in quotes below to OCR a different file.</span>
<span class="n">file</span> <span class="o">=</span> <span class="s2">&quot;./data/ocr_sample.jpg&quot;</span>

<span class="c1"># Get a file name--without the extension-- </span>
<span class="c1"># to use when we name the output file.</span>
<span class="n">fileName</span> <span class="o">=</span> <span class="n">file</span><span class="o">.</span><span class="n">rsplit</span><span class="p">(</span><span class="s1">&#39;/&#39;</span><span class="p">,</span> <span class="mi">1</span><span class="p">)[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>
<span class="n">fileName</span> <span class="o">=</span> <span class="n">fileName</span><span class="o">.</span><span class="n">rsplit</span><span class="p">(</span><span class="s1">&#39;.&#39;</span><span class="p">,</span> <span class="mi">1</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span>

<span class="c1"># Run OCR on an image file and save it as a PDF object (not file)</span>
<span class="c1"># within Python.</span>
<span class="n">pdf</span> <span class="o">=</span> <span class="n">pytesseract</span><span class="o">.</span><span class="n">image_to_pdf_or_hocr</span><span class="p">(</span><span class="n">file</span><span class="p">,</span> <span class="n">extension</span><span class="o">=</span><span class="s1">&#39;pdf&#39;</span><span class="p">)</span>

<span class="c1"># Create a new empty pdf.</span>
<span class="k">with</span> <span class="nb">open</span><span class="p">(</span><span class="n">fileName</span> <span class="o">+</span> <span class="s2">&quot;.pdf&quot;</span><span class="p">,</span> <span class="s1">&#39;w+b&#39;</span><span class="p">)</span> <span class="k">as</span> <span class="n">f</span><span class="p">:</span>
    
    <span class="c1"># Save the PDF object to the new empty PDF file.</span>
    <span class="n">f</span><span class="o">.</span><span class="n">write</span><span class="p">(</span><span class="n">pdf</span><span class="p">)</span>

<span class="c1"># Display a message to let us know the file has been created</span>
<span class="c1"># and the script successfully completed.</span>
<span class="nb">print</span><span class="p">(</span><span class="n">fileName</span><span class="p">,</span> <span class="s2">&quot;PDF successfully created.&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p>To open this file, use the file menu to select: <strong>File &gt; Open</strong>. You may then choose to view, edit, modify, or download the file to your local machine.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Output to html file (.html)</span>

<span class="c1"># File location</span>
<span class="c1"># You can change the filename in quotes below to OCR a different file.</span>
<span class="n">file</span> <span class="o">=</span> <span class="s2">&quot;./data/ocr_sample.jpg&quot;</span>

<span class="c1"># Get a file name--without the extension-- </span>
<span class="c1"># to use when we name the output file.</span>
<span class="n">fileName</span> <span class="o">=</span> <span class="n">file</span><span class="o">.</span><span class="n">rsplit</span><span class="p">(</span><span class="s1">&#39;/&#39;</span><span class="p">,</span> <span class="mi">1</span><span class="p">)[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>
<span class="n">fileName</span> <span class="o">=</span> <span class="n">fileName</span><span class="o">.</span><span class="n">rsplit</span><span class="p">(</span><span class="s1">&#39;.&#39;</span><span class="p">,</span> <span class="mi">1</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span>

<span class="c1"># Run OCR on an image file and save it as an HTML object (not file)</span>
<span class="c1"># within Python.</span>
<span class="n">hocr</span> <span class="o">=</span> <span class="n">pytesseract</span><span class="o">.</span><span class="n">image_to_pdf_or_hocr</span><span class="p">(</span><span class="n">file</span><span class="p">,</span> <span class="n">extension</span><span class="o">=</span><span class="s1">&#39;hocr&#39;</span><span class="p">)</span>

<span class="c1"># Create a new empty HTML file. Open it in &quot;w+b&quot; mode.</span>
<span class="c1"># &quot;w+b&quot; is a mode that tells Python to write whatever</span>
<span class="c1"># data we give to a file in binary mode--meaning that </span>
<span class="c1"># it will not apply any encoding or try to translate</span>
<span class="c1"># a non-ASCII character to an ASCII character.</span>
<span class="k">with</span> <span class="nb">open</span><span class="p">(</span><span class="n">fileName</span> <span class="o">+</span> <span class="s2">&quot;.html&quot;</span><span class="p">,</span> <span class="s1">&#39;w+b&#39;</span><span class="p">)</span> <span class="k">as</span> <span class="n">f</span><span class="p">:</span>
    
    <span class="c1"># Save the PDF object to the new empty PDF file.</span>
    <span class="n">f</span><span class="o">.</span><span class="n">write</span><span class="p">(</span><span class="n">hocr</span><span class="p">)</span>

<span class="c1"># Display a message to let us know the file has been created</span>
<span class="c1"># and the script successfully completed.</span>
<span class="nb">print</span><span class="p">(</span><span class="n">fileName</span><span class="p">,</span> <span class="s2">&quot;HTML successfully created.&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p>To open this file, use the file menu to select: <strong>File &gt; Open</strong>. You may then choose to view, edit, modify, or download the file to your local machine.</p>
</div>
<div class="section" id="languages">
<h3>Languages<a class="headerlink" href="#languages" title="Permalink to this headline">¶</a></h3>
<p>If we do not include <code class="docutils literal notranslate"><span class="pre">lang=&quot;eng&quot;</span></code> when we run the above code, Tesseract will <em>assume</em> English. Run the following to get a list of all the language codes. <a class="reference external" href="https://tesseract-ocr.github.io/tessdoc/Data-Files-in-different-versions.html">A table of these is available here.</a></p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Display a list of languages in their 3-letter codes supported by Tesseract.</span>
<span class="nb">print</span><span class="p">(</span><span class="n">pytesseract</span><span class="o">.</span><span class="n">get_languages</span><span class="p">(</span><span class="n">config</span><span class="o">=</span><span class="s1">&#39;&#39;</span><span class="p">))</span>
</pre></div>
</div>
</div>
</div>
<hr class="docutils" />
<h3 style="color:red; display:inline">Try it! &lt; / &gt; </h3>
<p>Try OCR’ing the first page from Gabriel Garcia-Marquez’s <em>Cien Años de Soledad</em> (<em>One Hundred Years of Solitude</em>):</p>
<p><img alt="The first page from one hundred years of solitude" src="https://ithaka-labs.s3.amazonaws.com/static-files/images/tdm/tdmdocs/cien-an%CC%83os-de-soledad.png" /></p>
<hr class="docutils" />
<p>The following code will download the file automatically. Can you figure out how to change the <a class="reference external" href="https://tesseract-ocr.github.io/tessdoc/Data-Files-in-different-versions.html">3-letter language code</a> to match the language in the text?</p>
<hr class="docutils" />
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Install Spanish language support</span>
<span class="c1"># Change `spa` to match the language code of your choice</span>
<span class="o">!</span>apt-get install tesseract-ocr-deu
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Language installed.&#39;</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Open a specific image file, convert the text in the image to computer-readable text (OCR),</span>
<span class="c1"># and then print the results for us to see here.</span>

<span class="c1"># Try changing the language paramater to &#39;eng&#39;. Does it change the output?</span>
<span class="nb">print</span><span class="p">(</span><span class="n">pytesseract</span><span class="o">.</span><span class="n">image_to_string</span><span class="p">(</span><span class="n">Image</span><span class="o">.</span><span class="n">open</span><span class="p">(</span><span class="s2">&quot;CHANGE-THE-FILENAME-HERE&quot;</span><span class="p">),</span> <span class="n">lang</span><span class="o">=</span><span class="s2">&quot;CHANGE-LANGUAGE-CODE&quot;</span><span class="p">))</span>
</pre></div>
</div>
</div>
</div>
<hr class="docutils" />
<h3 style="color:red; display:inline">Try it! &lt; / &gt; </h3>
<p>Try OCR’ing a page with multiple languages, such as The Bible in Hindi: (<a class="reference external" href="https://archive.org/details/holybibleinhindi00alla">Source</a>). The syntax will be <code class="docutils literal notranslate"><span class="pre">lang=&quot;lan+gua&quot;</span></code> – replace <code class="docutils literal notranslate"><span class="pre">lan</span></code> and <code class="docutils literal notranslate"><span class="pre">gua</span></code> with the correct language codes.</p>
<p>Can you figure out how to bring the document into the Analytics Lab and OCR with two languages?</p>
<p><strong>Note:</strong> The first language will be the “primary” language. Try changing the order of the languages to see how the output changes.</p>
<hr class="docutils" />
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Open a specific image file, convert the text in the image to computer-readable text (OCR),</span>
<span class="c1"># and then print the results for us to see here.</span>

<span class="c1"># REPLACE THE FILE NAME with one of the sample files above. (bible.png)</span>
<span class="c1"># REPLACE THE LANGUAGE attribute with the correct language code(s).</span>
<span class="nb">print</span><span class="p">(</span><span class="n">pytesseract</span><span class="o">.</span><span class="n">image_to_string</span><span class="p">(</span><span class="n">Image</span><span class="o">.</span><span class="n">open</span><span class="p">(</span><span class="s2">&quot;REPLACE_THIS_FILE_NAME.jpg&quot;</span><span class="p">),</span> <span class="n">lang</span><span class="o">=</span><span class="s2">&quot;lan+gua&quot;</span><span class="p">))</span>
</pre></div>
</div>
</div>
</div>
</div>
</div>
<div class="section" id="practice-apply-to-your-own-files">
<h2>Practice: Apply to your own files<a class="headerlink" href="#practice-apply-to-your-own-files" title="Permalink to this headline">¶</a></h2>
<p>Use the following code blocks to try OCR’ing various texts. You could use your own files containing digitized texts or locate files to try via <a class="reference external" href="https://www.jstor.org/">JStor</a>, the <a class="reference external" href="https://archive.org/">Internet Archive</a>, <a class="reference external" href="https://chroniclingamerica.loc.gov/">Chronicling America</a> or other resources. Try texts in different languages, fonts or types, formats, layouts, etc.</p>
<div class="section" id="upload-your-selected-text-s-to-the-data-folder-in-your-space-in-the-constellate-analytics-lab">
<h3>1. Upload your selected text(s) to the <code class="docutils literal notranslate"><span class="pre">data/</span></code> folder in your space in the Constellate Analytics Lab:<a class="headerlink" href="#upload-your-selected-text-s-to-the-data-folder-in-your-space-in-the-constellate-analytics-lab" title="Permalink to this headline">¶</a></h3>
<ul class="simple">
<li><p>Make sure that the texts you select are stored in an image (.jpg, .png, .tiff) format. If you have selected a text with multiple pages, make sure each page is stored in a separate file. <em>If you have PDF files and are not sure how to generate images from them, bring them to Lesson 02. We’ll be looking at how to generate image files together during the lesson.</em></p></li>
</ul>
<ol class="simple">
<li><p>Select <strong>File &gt; Open</strong></p></li>
<li><p>Navigate to the <code class="docutils literal notranslate"><span class="pre">data/</span></code> folder.</p></li>
<li><p>Select “Upload” and then locate and select the file on your local machine.</p></li>
<li><p>If the filename is long, rename it to something simple.</p></li>
<li><p><em>Click “Upload” again to start the upload.</em></p></li>
</ol>
</div>
<div class="section" id="perform-ocr-on-your-image-file">
<h3>2. Perform OCR on your image file.<a class="headerlink" href="#perform-ocr-on-your-image-file" title="Permalink to this headline">¶</a></h3>
<p>Use the code blocks above or start fresh below. Change the language attribute to match the text’s language. Try out the various settings we looked at above.</p>
<p>Below, make sure to replace “FOLDER NAME/FILE NAME” with your folder name and specific file name. For example, <code class="docutils literal notranslate"><span class="pre">./data/MY_FILE_NAME.jpg</span></code>.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Name the image file. (Assign it to a variable.)</span>
<span class="c1"># REPLACE THE FOLDER AND FILE NAME BELOW.</span>
<span class="n">file</span> <span class="o">=</span> <span class="s2">&quot;./data/MY_FILE_NAME.jpg&quot;</span>

<span class="n">custom_oem_psm_config</span> <span class="o">=</span> <span class="sa">r</span><span class="s1">&#39;--oem 3 --psm 3&#39;</span>

<span class="c1"># Open the file named above. </span>
<span class="c1"># While it&#39;s open, do several things:</span>
<span class="k">with</span> <span class="nb">open</span><span class="p">(</span><span class="n">file</span><span class="p">,</span> <span class="s1">&#39;rb&#39;</span><span class="p">)</span> <span class="k">as</span> <span class="n">inputFile</span><span class="p">:</span>
        
    <span class="c1"># Read the file using PIL&#39;s Image module.</span>
    <span class="n">img</span> <span class="o">=</span> <span class="n">Image</span><span class="o">.</span><span class="n">open</span><span class="p">(</span><span class="n">inputFile</span><span class="p">)</span>
    
    <span class="c1"># Run OCR on the open file.</span>
    <span class="n">ocrText</span> <span class="o">=</span> <span class="n">pytesseract</span><span class="o">.</span><span class="n">image_to_string</span><span class="p">(</span><span class="n">img</span><span class="p">,</span> <span class="n">lang</span><span class="o">=</span><span class="s2">&quot;eng&quot;</span><span class="p">,</span> <span class="n">config</span><span class="o">=</span><span class="n">custom_oem_psm_config</span><span class="p">)</span>
        
<span class="c1"># Get a file name--without the extension-- </span>
<span class="c1"># to use when we name the output file.</span>
<span class="n">fileName</span> <span class="o">=</span> <span class="n">file</span><span class="o">.</span><span class="n">rsplit</span><span class="p">(</span><span class="s1">&#39;/&#39;</span><span class="p">,</span> <span class="mi">1</span><span class="p">)[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>
<span class="n">fileName</span> <span class="o">=</span> <span class="n">fileName</span><span class="o">.</span><span class="n">rsplit</span><span class="p">(</span><span class="s1">&#39;.&#39;</span><span class="p">,</span> <span class="mi">1</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span>

<span class="c1"># The image file above will be closed before moving on to this line.</span>
<span class="c1"># The OCR&#39;ed text has been pulled from the image and stored in</span>
<span class="c1"># a Python variable for us to continue to use.</span>

<span class="c1"># Create and open a new text file, name it to match its input file,</span>
<span class="c1"># declare its encoding to be UTF-8 so that it correctly outputs</span>
<span class="c1"># non-ASCII characters,</span>
<span class="k">with</span> <span class="nb">open</span><span class="p">(</span><span class="n">fileName</span> <span class="o">+</span> <span class="s2">&quot;.txt&quot;</span><span class="p">,</span> <span class="s2">&quot;w&quot;</span><span class="p">,</span> <span class="n">encoding</span><span class="o">=</span><span class="s2">&quot;utf-8&quot;</span><span class="p">)</span> <span class="k">as</span> <span class="n">outFile</span><span class="p">:</span>
        
    <span class="c1"># and write the OCR&#39;ed text to the file.</span>
    <span class="n">outFile</span><span class="o">.</span><span class="n">write</span><span class="p">(</span><span class="n">ocrText</span><span class="p">)</span>

<span class="c1"># Display a message to let us know the file has been created</span>
<span class="c1"># and the script successfully completed.</span>
<span class="nb">print</span><span class="p">(</span><span class="n">fileName</span><span class="p">,</span> <span class="s2">&quot;text file successfully created.&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
</div>
</div>
<div class="section" id="resources-a-class-anchor-id-resources-a">
<h2>Resources <a class="anchor" id="resources"></a><a class="headerlink" href="#resources-a-class-anchor-id-resources-a" title="Permalink to this headline">¶</a></h2>
<hr class="docutils" />
<div class="section" id="jupyter-notebooks-tutorials-reference">
<h3>Jupyter Notebooks Tutorials &amp; Reference<a class="headerlink" href="#jupyter-notebooks-tutorials-reference" title="Permalink to this headline">¶</a></h3>
<ul class="simple">
<li><p><a class="reference external" href="https://jupyter-notebook.readthedocs.io/en/stable/notebook.html">Jupyter Notebooks documentation</a></p></li>
<li><p>Jupyter Notebooks keyboard shortcuts: press Esc+H to show a full list.</p></li>
<li><p><a class="reference external" href="https://www.ibm.com/docs/en/watson-studio-local/1.2.3?topic=notebooks-markdown-jupyter-cheatsheet">“Markdown for Jupyter notebooks cheatsheet.”</a> <em>IBM Watson Studio Local.</em></p></li>
<li><p>Olivia Smith. <a class="reference external" href="https://www.datacamp.com/community/tutorials/markdown-in-jupyter-notebook">“Markdown in Jupyter Notebook.”</a> <em>Data Camp.</em></p></li>
</ul>
</div>
<div class="section" id="readings-on-ocr">
<h3>Readings on OCR<a class="headerlink" href="#readings-on-ocr" title="Permalink to this headline">¶</a></h3>
<ul class="simple">
<li><p>Algun, Selcuk. 2018. [“Review for Tesseract and Kraken OCR for text recognition.”](Review for Tesseract and Kraken OCR for text recognition) <em>Data Driven Investor.</em></p></li>
<li><p>Bakker, Rebecca. <a class="reference external" href="https://digitalcommons.fiu.edu/cgi/viewcontent.cgi?article=1047&amp;context=glworks">“OCR for Digital Collections.”</a> <em>FIU Digital Commons.</em></p></li>
<li><p>Baumman, Ryan. <a class="reference external" href="https://ryanfb.github.io/etc/2015/03/16/automatic_evaluation_of_ocr_quality.html">“Automatic evaluation of OCR quality.”</a> <em>/etc.</em></p></li>
<li><p>Cordell, R. 2017. <a class="reference external" href="https://ryancordell.org/research/qijtb-the-raven/">“Q i-jtb the Raven”: Taking Dirty OCR Seriously.”</a> <em>Book History</em>, 20, 188-225.</p></li>
<li><p>Cordell, Ryan. 2019. <a class="reference external" href="https://ryancordell.org/research/why-ocr/">“Why You (A Humanist) Should Care About Optical Character Recognition.”</a> <em>Ryan Cordell.</em></p></li>
<li><p>Coyle, Karen. <a class="reference external" href="https://kcoyle.blogspot.com/2012/04/digital-urtext.html">“Digital Urtext.”</a> <em>Coyle’s InFormation.</em></p></li>
<li><p>Hawk, Brandon W. <a class="reference external" href="https://brandonwhawk.net/2015/04/20/ocr-and-medieval-manuscripts-establishing-a-baseline/">“OCR and Medieval Manuscripts: Establishing a Baseline.”</a> <em>Brandon W. Hawk.</em> (This post is a comparison of ABBYY FineReader &amp; Adobe Acrobat OCR technologies as applied to medieval texts.)</p></li>
<li><p>Holley, Rose. 2009. <a class="reference external" href="http://www.dlib.org/dlib/march09/holley/03holley.html">How Good Can It Get? Analysing and Improving OCR Accuracy in Large Scale Historic Newspaper Digitisation Programs,”</a> <em>D-Lib Magazine</em> 15, no. 3/4.</p></li>
<li><p>Milligan, I. 2013. <a class="reference external" href="https://www.muse.jhu.edu/article/527016">“Illusionary Order: Online Databases, Optical Character Recognition, and Canadian History, 1997–2010.</a> <em>The Canadian Historical Review</em> 94(4), 540-569.</p></li>
<li><p>Smith, David, and Ryan Cordell. 2018. <a class="reference external" href="http://hdl.handle.net/2047/D20297452">“A Research Agenda for Historical and Multilingual Optical Character Recognition.”</a></p></li>
<li><p>Smith, Ray. 2007. <a class="reference external" href="https://tesseract-ocr.github.io/docs/tesseracticdar2007.pdf">“An Overview of the Tesseract OCR Engine.”</a></p></li>
<li><p>Smith, Ray, Daria Antonova, and Dar-Shyang Lee. 2009. <a class="reference external" href="https://dl.acm.org/doi/10.1145/1577802.1577804">“Adapting the Tesseract open source OCR engine for multilingual OCR.”</a> MOCR ‘09: Proceedings of the International Workshop on Multilingual OCR.</p></li>
<li><p>Tanner, Simon. <a class="reference external" href="https://www.kb.nl/sites/default/files/docs/OCRFeasibility_final.pdf">“Deciding whether Optical Character Recognition is feasible.”</a> <em>King’s Digital Consultancy Services.</em></p></li>
</ul>
</div>
<div class="section" id="ocr-tutorials-reference">
<h3>OCR Tutorials &amp; Reference<a class="headerlink" href="#ocr-tutorials-reference" title="Permalink to this headline">¶</a></h3>
<p><em>The following is a list of tutorials that include different scholars’ approaches to OCR. Some also use Tesseract, but most use different scripting or programming languages. There is no single best way to do OCR, so if you have the time they worth trying to see which works best for your project.</em></p>
<ul class="simple">
<li><p>Aidan. <a class="reference external" href="https://medhieval.com/classes/hh2019/blog/ocr-with-python/">“OCR with Python.”</a> <em>Hacking the Humanities 2019.</em></p></li>
<li><p>Akhlaghi, Andrew. <a class="reference external" href="http://programminghistorian.org/en/lessons/OCR-and-Machine-Translation">“OCR and Machine Translation.”</a> <em>The Programming Historian.</em> (Note that this tutorial uses Tesseract but works with the bash scripting language instead of Python.)</p></li>
<li><p>Baumman, Ryan. <a class="reference external" href="https://ryanfb.github.io/etc/2014/11/13/command_line_ocr_on_mac_os_x.html">“Command-Line OCR with Tesseract on Mac OS X.”</a> <em>/etc.</em></p></li>
<li><p>Dull, Joshua. <a class="reference external" href="https://github.com/JoshuaDull/Text-Recognition-Introduction/">“Text Recognition with Adobe Acrobat and ABBYY FineReader.”</a></p></li>
<li><p>Graham, Shawn. <a class="reference external" href="https://electricarchaeology.ca/2014/07/15/doing-ocr-within-r/">“Extracting Text from PDFs; Doing OCR; all within R.”</a> <em>Electric Archaeology.</em> (This blog post describes a method for OCR using the R programming language.)</p></li>
<li><p>Mähr, Moritz. <a class="reference external" href="https://programminghistorian.org/en/lessons/working-with-batches-of-pdf-files">“Working with batches of PDF files.”</a> <em>The Programming Historian.</em> (Note that this tutorial uses Tesseract and works in the command line without Python.)</p></li>
<li><p>Shperber, Gidi. <a class="reference external" href="https://towardsdatascience.com/a-gentle-introduction-to-ocr-ee1469a201aa">“A gentle introduction to OCR.”</a> <em>Toward Data Science.</em> October 22, 2018.</p></li>
<li><p>Tarnopol, Rebecca. <a class="reference external" href="https://business.tutsplus.com/tutorials/how-to-ocr-documents-for-free-in-google-drive--cms-20460">“How to OCR Documents for Free in Google Drive.”</a> <em>TutsPlus.</em></p></li>
<li><p><a class="reference external" href="https://github.com/madmaze/pytesseract"><strong>PyTesseract documentation</strong></a></p></li>
<li><p><a class="reference external" href="https://tesseract-ocr.github.io/"><strong>Tesseract documentation</strong></a></p></li>
</ul>
</div>
<div class="section" id="additional-reading">
<h3>Additional Reading<a class="headerlink" href="#additional-reading" title="Permalink to this headline">¶</a></h3>
<ul class="simple">
<li><p>Rockwell, Geoffrey, and Stéfan Sinclair. 2016. <a class="reference external" href="http://hermeneuti.ca/"><em>Hermeneutica: Computer Assisted Interpretation in the Humanities.</em></a></p></li>
<li><p>Underwood, Ted. <a class="reference external" href="https://tedunderwood.com/2011/10/07/the-challenges-of-digital-work-on-early-19c-collections/">“The challenges of digital work on early-19c collections.”</a> <em>The Stone and the Shell.</em></p></li>
<li><p><a class="reference external" href="https://cordis.europa.eu/project/id/600707/results">TranScriptorium’s handwritten text recognition project results.</a></p></li>
<li><p><a class="reference external" href="https://readcoop.eu/transkribus/howto/how-to-transcribe-documents-with-transkribus-introduction/">“How to Transcribe Documents with Transkribus - Introduction.”</a> <em>Read Coop.</em></p></li>
<li><p><strong><a class="reference external" href="https://copyright.columbia.edu/basics/fair-use.html">Basics of Fair Use</a></strong> from Columbia University.</p></li>
</ul>
</div>
<div class="section" id="additional-tutorials-thanks-to-everyone-who-contributed">
<h3>Additional Tutorials – Thanks to everyone who contributed!<a class="headerlink" href="#additional-tutorials-thanks-to-everyone-who-contributed" title="Permalink to this headline">¶</a></h3>
<ul class="simple">
<li><p><a class="reference external" href="https://towardsdatascience.com/social-network-analysis-from-theory-to-applications-with-python-d12e9a34c2c7">“Social Network Analysis from Theory to Applications with Python.”</a> <em>Toward Data Science.</em></p></li>
<li><p><a class="reference external" href="https://nkelber.github.io/tapi2021/book/courses/pandas.html">TAP Institute Pandas course materials</a> – for “cleaning” data using Python.</p></li>
<li><p><a class="reference external" href="https://programminghistorian.org/en/lessons/cleaning-data-with-openrefine">“Cleaning Data with Open Refine.”</a> <em>The Programming Historian.</em></p></li>
<li><p><a class="reference external" href="https://automatetheboringstuff.com/chapter8/">Working with Files in Python</a> from <em>Automate the Boring Stuff</em>.</p></li>
</ul>
</div>
</div>
</div>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            kernelName: "python3",
            path: "./."
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

              </div>
              
        
            



<div class='prev-next-bottom'>
    

</div>
        
        </div>
    </div>
    <footer class="footer">
    <div class="container">
      <p>
        
          By Nathan Kelber and Ted Lawless<br/>
        
          <div class="extra_footer">
            <div>
<a href="https://creativecommons.org/licenses/by/2.0/"><img class="license" alt="Creative Commons License" src="https://ithaka-labs.s3.amazonaws.com/static-files/images/tdm/tdmdocs/CC_BY.png" /></a> The tutorials and workshop materials are licensed under a <a href="https://creativecommons.org/licenses/by/2.0/">Creative Commons BY License</a>.
</div>

          </div>
      </p>
    </div>
  </footer>
</main>


      </div>
    </div>
  
  <script src="_static/js/index.1c5a1a01449ed65a7b51.js"></script>

  
<script async="" src="https://www.google-analytics.com/analytics.js"></script>
<script>
                        window.ga = window.ga || function () {
                            (ga.q = ga.q || []).push(arguments) };
                        ga.l = +new Date;
                        ga('create', 'UA-125778965-3', 'auto');
                        ga('set', 'anonymizeIp', true);
                        ga('send', 'pageview');
                    </script>

  </body>
</html>